{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 応用演習2<パーセプトロン>\n",
        "\n",
        "\n",
        "---\n",
        "　この演習では、ニューラルネットワークの原点であるパーセプトロンの実験を行います。この演習を通して、層を深くすることと活性化関数の効果を体感できます。\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        ">　パーセプトロンは、人間の脳機能をモデル化したもので、複数の信号を受けて一つの信号を出力することができます。入力信号*x*と重み*w*の積の総和を出力とします。以下に、シンプルなパーセプトロンの図を示します。\n"
      ],
      "metadata": {
        "id": "zFFApRd5_29N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1WVr987sZV5xU3ul6iPjIiLFjzskFhqLD\" alt=\"\" title=\"neuron\" width=30%>"
      ],
      "metadata": {
        "id": "oKtuzRdMUepw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> このパーセプトロンをPytorchを使って再現します。\n",
        "\n",
        "\n",
        "\n",
        ">>(厳密には、単純パーセプトロンとは異なるモデルです。初期のパーセプトロンでは、出力に対してしきい値関数を用いて1または0を決定します。しかし、しきい値関数は微分可能でない点があるのでバックプロパゲーションと相性が良くありません。)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sbIGGpwaWDTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "> 初めに、使用するモジュールのインポートします。\n",
        "\n"
      ],
      "metadata": {
        "id": "vwPtCClnkoMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "gR1Rb6XrW_kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> 次の部分で、パーセプトンもどきを定義します。nn.Linear()で入力データ*x*に重みを掛け合わせます。続く、Sigmoid関数は最初期のパーセプトロンでは用いられていませんが、1.バックプロパゲーションと2.精度を安定させるために使用します。\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2FzHsB0XlHV0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCjRy6WU_Rmh"
      },
      "outputs": [],
      "source": [
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #全結合層(入力2,出力1)\n",
        "        self.fc = nn.Linear(2,1)\n",
        "        #活性化関数(sigmoid関数)\n",
        "        self.act = nn.Sigmoid()\n",
        "    def forward(self,x):\n",
        "        y = self.fc(x)\n",
        "        y = self.act(y)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> 次に、入力データ*x*で用いるANDゲートの入力と出力を設定します。\n",
        "\n"
      ],
      "metadata": {
        "id": "t0F1aWFeDMJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### AND gate ####\n",
        "and_data = []\n",
        "and_data.append(([0,0]))  # 0&0=0\n",
        "and_data.append(([0,1]))  # 0&1=0\n",
        "and_data.append(([1,0]))  # 1&0=0\n",
        "and_data.append(([1,1]))  # 1&1=1\n",
        "\n",
        "### 正解ラベル ###\n",
        "and_target_label = [0,0,0,1]\n",
        "\n",
        "for i in range(len(and_data)):\n",
        "    print(f\"Input {and_data[i]}, Output {and_target_label[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JokZ8YiYW6Ja",
        "outputId": "2bd4f62b-82fe-4d30-95b9-5d54d41c1643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input [0, 0], Output 0\n",
            "Input [0, 1], Output 0\n",
            "Input [1, 0], Output 0\n",
            "Input [1, 1], Output 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> 続いては、データセット(入力するデータを集約してネットワークに渡す)を作成します。自作のデータセットは、今後必須になってくるので後ほど理解できるようになると良いと思います。\n",
        "\n"
      ],
      "metadata": {
        "id": "eHNH88WlDjW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self,data,label):\n",
        "        self.data_info = list()\n",
        "        for idx,data in enumerate(data):\n",
        "            _data = torch.tensor(data).to(torch.float32)\n",
        "            _label = torch.tensor(label)\n",
        "            self.data_info.append((_data,_label[idx]))\n",
        "    def __getitem__(self,index):\n",
        "        data,label = self.data_info[index]\n",
        "        return data, label\n",
        "    def __len__(self):\n",
        "        return len(self.data_info)"
      ],
      "metadata": {
        "id": "dtAILpnHb7Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MyDataset(and_data, and_target_label)\n",
        "dataloader = DataLoader(dataset, batch_size=4)"
      ],
      "metadata": {
        "id": "Xg1WehFBkJgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> 以下は、学習要件の定義と訓練部分です。\n",
        "\n",
        "専門用語 :\n",
        "*   エポック\n",
        " *  エポックとは、ハイパーパラメータ(手動で設定する値)の一種で、訓練データ全体を何回繰り返して学習を行うかを決めます。\n",
        "*   損失関数\n",
        " *  正解の値と、モデルによる予測結果の誤差を数値化します。\n",
        "*   Optimizer\n",
        "  * 械学習モデルは、試行錯誤を繰り返して重みを更新し、損失0を目指しますが、Optimzerは損失を効率よく0に近づける最適化アルゴリズムです。例をあげると、勾配降下法では、勾配(傾き)を求めて斜面を下る方向に重みを更新し、損失の最小値を求めます。\n",
        "*   バックプロパゲーション\n",
        " * 勾配降下法では、勾配を求めるために、微分する必要があります。バックプロパゲーションでは、出力層から入力層に向かって誤差の偏微分を伝番していき、各層の微分を求めます。\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vor2HIchHBKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#エポック数\n",
        "num_epochs = 50\n",
        "#ネットワークの\n",
        "net = SimpleNet()\n",
        "# 損失関数の定義\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    total = 0\n",
        "    for i,(x,label) in enumerate(dataloader):\n",
        "        y_ = net(x)\n",
        "        label_tensor = torch.unsqueeze(label,dim=1).to(torch.float32)\n",
        "        loss = criterion(y_, label_tensor)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # 出力に対して、しきい値を設定し、0か1の出力を得ます。\n",
        "        pred = torch.heaviside((y_*2-1),torch.zeros_like(y_))\n",
        "        correct += (pred.squeeze() == label).sum().item()\n",
        "        total += x.shape[0]\n",
        "\n",
        "    print('Epoch : {}\\t'.format(epoch),\n",
        "        'Loss : {:.3f}\\t'.format(total_loss/(i+1)),\n",
        "        'Acc : {}\\t'.format((correct/total)*100))\n",
        "    if ((epoch+1) % 50 == 0):\n",
        "        for i in range(x.shape[0]):\n",
        "            print('In : {}\\t Out : {}\\t Target : {}\\n'.format(x[i].numpy(), pred[i].detach().numpy(),label[i]))\n"
      ],
      "metadata": {
        "id": "XNj9siZucb-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> 学習率(learning rate)、エポック数、シード値がかみ合うと、正解率100%が得られます。\n",
        "\n"
      ],
      "metadata": {
        "id": "0d3-tFx1TAZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> 次に、XORゲートの出力を予測します。\n",
        "\n"
      ],
      "metadata": {
        "id": "Ej3WOGl8TOlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xor_data = []\n",
        "xor_data.append((0,0))\n",
        "xor_data.append((0,1))\n",
        "xor_data.append((1,0))\n",
        "xor_data.append((1,1))\n",
        "xor_target_label = [0,1,1,0]"
      ],
      "metadata": {
        "id": "FaUVqR8ufMk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> 先ほどと同様に、モデルの訓練をしてみましょう。\n",
        "\n"
      ],
      "metadata": {
        "id": "qVqwLieSTeDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MyDataset(xor_data, xor_target_label)\n",
        "dataloader = DataLoader(dataset, batch_size=4)\n",
        "#エポック数\n",
        "num_epochs = 50\n",
        "#ネットワークの\n",
        "net = SimpleNet()\n",
        "# 損失関数の定義\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    total = 0\n",
        "    for i,(x,label) in enumerate(dataloader):\n",
        "        y_ = net(x)\n",
        "        label_tensor = torch.unsqueeze(label,dim=1).to(torch.float32)\n",
        "        loss = criterion(y_, label_tensor)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # 出力に対して、しきい値を設定し、0か1の出力を得ます。\n",
        "        pred = torch.heaviside((y_*2-1),torch.zeros_like(y_))\n",
        "        correct += (pred.squeeze() == label).sum().item()\n",
        "        total += x.shape[0]\n",
        "\n",
        "    if (epoch % 5==0):\n",
        "        print('Epoch : {}\\t'.format(epoch),\n",
        "            'Loss : {:.3f}\\t'.format(total_loss/(i+1)),\n",
        "            'Acc : {}\\t'.format((correct/total)*100))\n",
        "\n",
        "\n",
        "for i in range(x.shape[0]):\n",
        "    print('In : {}\\t Out : {}\\t Target : {}\\n'.format(x[i].numpy(), pred[i].detach().numpy(),label[i]))"
      ],
      "metadata": {
        "id": "9cxOMlv3TYZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "> 高い正解率を得るのが難しいですね。これは、ネットワークの根本的な問題が原因です。パーセプトロンは、出力を*y = w1x1 + w2x2*で表現しますが、これだと直線で分割できる問題しか解決できません。\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=11-jZyRpaVZN1npZbBlynfjs7VR-H1kqy\" alt=\"\" title=\"neuron\" width=60%>\n",
        "\n",
        "\n",
        "> これを解決する方法は、1. 層を増やす、2. 活性化関数(非線形関数)を加える ことです。これにより、非線形分離ができるようになり、XORの予測ができます。それでは、SimpleNet()を改良してみましょう。\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8WQouiTTTuXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLayerNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(2,4)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(4,1)\n",
        "        self.act = nn.Sigmoid()\n",
        "    def forward(self,x):\n",
        "        y = self.fc1(x)\n",
        "        y = self.relu(y)\n",
        "        y = self.fc2(y)\n",
        "        y = self.act(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "SrD7p7HxjeNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> データセットをXOR用に変更します。\n",
        "\n"
      ],
      "metadata": {
        "id": "w7W8UZ3YbKLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MyDataset(xor_data, xor_target_label)\n",
        "dataloader = DataLoader(dataset, batch_size=4)"
      ],
      "metadata": {
        "id": "gjCUICrMuLEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#エポック数\n",
        "num_epochs = 50\n",
        "#ネットワークの\n",
        "net = MultiLayerNet()\n",
        "# 損失関数の定義\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    total = 0\n",
        "    for i,(x,label) in enumerate(dataloader):\n",
        "        y_ = net(x)\n",
        "        label_tensor = torch.unsqueeze(label,dim=1).to(torch.float32)\n",
        "        loss = criterion(y_, label_tensor)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # 出力に対して、しきい値を設定し、0か1の出力を得ます。\n",
        "        pred = torch.heaviside((y_*2-1),torch.zeros_like(y_))\n",
        "        correct += (pred.squeeze() == label).sum().item()\n",
        "        total += x.shape[0]\n",
        "    if ((epoch+1)%5==0):\n",
        "        print('Epoch : {}\\t'.format(epoch),\n",
        "            'Loss : {:.3f}\\t'.format(total_loss/(i+1)),\n",
        "            'Acc : {}\\t'.format((correct/total)*100))\n",
        "\n",
        "for i in range(x.shape[0]):\n",
        "    print('In : {}\\t Out : {}\\t Target : {}\\n'.format(x[i].numpy(), pred[i].detach().numpy(),label[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiRrh1tcaajA",
        "outputId": "ea9ec1c1-882f-4475-f52e-f7f760a05b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 4\t Loss : 0.681\t Acc : 50.0\t\n",
            "Epoch : 9\t Loss : 0.679\t Acc : 50.0\t\n",
            "Epoch : 14\t Loss : 0.677\t Acc : 75.0\t\n",
            "Epoch : 19\t Loss : 0.674\t Acc : 75.0\t\n",
            "Epoch : 24\t Loss : 0.671\t Acc : 75.0\t\n",
            "Epoch : 29\t Loss : 0.669\t Acc : 100.0\t\n",
            "Epoch : 34\t Loss : 0.666\t Acc : 100.0\t\n",
            "Epoch : 39\t Loss : 0.663\t Acc : 100.0\t\n",
            "Epoch : 44\t Loss : 0.659\t Acc : 100.0\t\n",
            "Epoch : 49\t Loss : 0.656\t Acc : 100.0\t\n",
            "In : [0. 0.]\t Out : [0.]\t Target : 0\n",
            "\n",
            "In : [0. 1.]\t Out : [1.]\t Target : 1\n",
            "\n",
            "In : [1. 0.]\t Out : [1.]\t Target : 1\n",
            "\n",
            "In : [1. 1.]\t Out : [0.]\t Target : 0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> ネットワークや、学習率、エポック数を変更して、正解率100%を目指しましょう。\n",
        "\n"
      ],
      "metadata": {
        "id": "zV0S96aqbv7p"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tudb9mhBuV5j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}